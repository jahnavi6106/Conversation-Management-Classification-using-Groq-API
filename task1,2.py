# -*- coding: utf-8 -*-
"""task1,2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mibg877fbflJo7otO_5nTsmKPIQov9pX
"""

import os
import json
import time
import textwrap
import re
from typing import List, Dict, Any, Optional
import requests

# ----------------------------
# Configuration
# ----------------------------
GROQ_API_KEY = os.getenv('GROQ_API_KEY', '<INSERT_YOUR_GROQ_API_KEY_HERE>')
GROQ_API_BASE = os.getenv('GROQ_API_BASE', 'https://api.groq.ai/v1')
MODEL = os.getenv('GROQ_MODEL', 'gpt-4o-mini')  # change if your provider suggests another model

HEADERS = {
    'Content-Type': 'application/json',
    'Authorization': f'Bearer {GROQ_API_KEY}'
}

# Helper to call Groq(OpenAI-compatible) chat completions endpoint
def groq_chat(messages: List[Dict[str, str]],
              functions: Optional[List[Dict[str, Any]]] = None,
              function_call: Optional[Any] = None,
              max_tokens: int = 800,
              temperature: float = 0.2) -> Dict[str, Any]:

    if not GROQ_API_KEY or GROQ_API_KEY.startswith('<INSERT'):
        raise ValueError('GROQ_API_KEY not provided. Use a fallback or set the GROQ_API_KEY env var.')

    payload = {
        'model': MODEL,
        'messages': messages,
        'temperature': temperature,
        'max_tokens': max_tokens,
    }
    if functions is not None:
        payload['functions'] = functions
    if function_call is not None:
        payload['function_call'] = function_call

    url = f"{GROQ_API_BASE}/chat/completions"
    resp = requests.post(url, headers=HEADERS, json=payload, timeout=60)
    resp.raise_for_status()
    return resp.json()

# ----------------------------
# Task 1: Conversation History Manager
# ----------------------------
class ConversationHistory:
    def __init__(self):
        # messages are dicts like {"role": "user"/"assistant"/"system", "content": "..."}
        self.messages: List[Dict[str, str]] = []
        self.run_counter = 0

    def add_message(self, role: str, content: str):
        self.messages.append({'role': role, 'content': content})

    def get_last_n_turns(self, n: int) -> List[Dict[str, str]]:
        # A "turn" is a user+assistant pair. We'll approximate by taking last 2*n messages.
        take = 2 * n
        return self.messages[-take:] if take <= len(self.messages) else list(self.messages)

    def get_truncated_by_chars(self, max_chars: int) -> List[Dict[str, str]]:
        # Walk backwards and keep adding messages until length exceeds max_chars
        res = []
        total = 0
        for msg in reversed(self.messages):
            l = len(msg['content'])
            if total + l > max_chars and res:
                break
            res.insert(0, msg)
            total += l
        return res

    def summarize_history(self, summarization_prompt: Optional[str] = None) -> str:
        """Summarize entire history using the model. Returns the summary string and replaces history with it.

        Uses groq_chat. If API key missing, uses a simple local summarizer as fallback.
        """
        self.run_counter += 1
        prompt = summarization_prompt or (
            "Summarize the conversation history into concise bullet points and a 2-3 sentence summary. "
            "Keep important user preferences, actions, and open tasks."
        )

        messages = [
            {'role': 'system', 'content': 'You are a concise summarizer for chat histories.'},
            {'role': 'user', 'content': prompt + "\n\nConversation:\n" + '\n'.join([f"{m['role']}: {m['content']}" for m in self.messages])}
        ]

        try:
            resp = groq_chat(messages, max_tokens=400)
            # navigate response
            summary = resp['choices'][0]['message']['content']
        except Exception as e:
            # fallback — naive summarizer: take first sentence from each message and join as bullets
            bullets = []
            for m in self.messages[-20:]:
                s = re.split(r'[.?!]\s+', m['content'].strip())
                if s:
                    bullets.append(f"- ({m['role']}) {s[0].strip()}.")
            summary = "\n".join(bullets)

        # Replace conversation with a system message containing the summary + keep last user turn for context
        new_messages = [{'role': 'system', 'content': 'Conversation summary:\n' + summary}]
        # Keep the last user message and assistant message (if any) for continuity
        if self.messages:
            new_messages.extend(self.messages[-2:])
        self.messages = new_messages
        return summary

    def maybe_summarize_every_k(self, k: int) -> Optional[str]:
        """Call summarize_history after every k runs (where a run is a call to this function).
        Returns summary if performed, else None."""
        if k <= 0:
            return None
        if self.run_counter % k == 0:
            return self.summarize_history()
        return None

# ----------------------------
# Demonstration helper for Task 1
# ----------------------------
def demo_task1():
    print('\n--- Task 1 Demo: Conversation history + summarization ---')
    conv = ConversationHistory()

    # feed multiple conversation samples
    samples = [
        ('user', 'Hi, I need help planning a healthy meal plan.'),
        ('assistant', 'Sure! What is your age, weight, and any dietary restrictions?'),
        ('user', 'I am 28, 70kg, vegetarian and I avoid dairy.'),
        ('assistant', 'Got it — vegetarian, no dairy. Any specific goals (weight loss, muscle gain)?'),
        ('user', 'I want to lose some fat and have more energy during the day.'),
        ('assistant', 'Great — I recommend a moderate calorie deficit with higher protein...'),
        ('user', "Also, I prefer quick recipes I can make in 30 minutes."),
    ]

    for role, text in samples:
        conv.add_message(role, text)

    print('\nFull conversation (raw):')
    for m in conv.messages:
        print(f"{m['role']}: {m['content']}")

    # Show truncation by turns
    print('\nTruncate by last 2 turns:')
    last_turns = conv.get_last_n_turns(2)
    for m in last_turns:
        print(f"{m['role']}: {m['content']}")

    # Show truncation by chars
    print('\nTruncate to ~100 chars:')
    trunc_chars = conv.get_truncated_by_chars(100)
    for m in trunc_chars:
        print(f"{m['role']}: {m['content']}")

    # Show periodic summarization after every k-th run
    k = 3
    print(f"\nDemonstrating summarization after every {k} runs. Running summaries 6 times (some may be no-ops).\n")
    # We'll append small messages and call maybe_summarize_every_k each time
    for i in range(1, 7):
        conv.add_message('user', f'Follow-up question #{i}: small clarification.')
        conv.run_counter += 1  # simulate a run; alternatively, call maybe_summarize_every_k after a real run
        summary = None
        if conv.run_counter % k == 0:
            summary = conv.summarize_history()
            print(f"--- Summarization triggered at run {conv.run_counter} ---")
            print(summary)
            print('\nCurrent stored messages after summarization:')
            for m in conv.messages:
                print(f"{m['role']}: {m['content']}")
        else:
            print(f"Run {conv.run_counter}: no summarization")

# ----------------------------
# Task 2: JSON Schema Classification & Extraction
# ----------------------------

# Define JSON Schema (simple representation for demonstration)
JSON_SCHEMA = {
    'name': {'type': 'string', 'description': 'Full name of the user'},
    'email': {'type': 'string', 'format': 'email', 'description': 'Email address'},
    'phone': {'type': 'string', 'description': 'Phone number (with country code if available)'},
    'location': {'type': 'string', 'description': 'City, state or country'},
    'age': {'type': 'integer', 'description': 'Age in years'},
}

# Build a functions-like schema for function calling (OpenAI-style)
FUNCTION_SCHEMA = [
    {
        'name': 'extract_contact_info',
        'description': 'Extract contact / personal info from a chat message',
        'parameters': {
            'type': 'object',
            'properties': {
                'name': {'type': 'string'},
                'email': {'type': 'string'},
                'phone': {'type': 'string'},
                'location': {'type': 'string'},
                'age': {'type': 'integer'},
            },
            'required': []
        }
    }
]

# Helper to call the function-calling style extraction
def extract_with_model(chat_text: str) -> Dict[str, Any]:
    messages = [
        {'role': 'system', 'content': 'You are a strict JSON extractor. Return only valid JSON matching the schema when possible.'},
        {'role': 'user', 'content': f'Extract name, email, phone, location, age from the following chat message: "{chat_text}"'}
    ]
    try:
        resp = groq_chat(messages, functions=FUNCTION_SCHEMA, function_call={'name': 'extract_contact_info'}, max_tokens=300)
        # The model should return a function call with arguments containing JSON
        choice = resp['choices'][0]
        msg = choice.get('message', {})
        # Depending on provider, function_call might be nested differently
        if 'function_call' in msg:
            args_text = msg['function_call'].get('arguments', '')
        else:
            # fallback: parse the content as JSON
            args_text = msg.get('content', '')
        # ensure json
        parsed = json.loads(args_text) if args_text else {}
        return parsed
    except Exception as e:
        # fallback local regex-based extractor for demonstration
        return local_fallback_extractor(chat_text)

# Local fallback extractor (simple and imperfect) for demo when API unavailable
def local_fallback_extractor(text: str) -> Dict[str, Any]:
    out = {}
    # name: look for patterns like "I'm John Doe" or "My name is John Doe"
    m = re.search(r"\b(?:my name is|i'm|i am)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)", text, re.IGNORECASE)
    if m:
        out['name'] = m.group(1).strip()
    # email
    me = re.search(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}", text)
    if me:
        out['email'] = me.group(0)
    # phone (simple)
    mp = re.search(r"(\+?\d{1,3}[-.\s]?)?(?:\(?\d{3}\)?[-.\s]?)?\d{3}[-.\s]?\d{4}", text)
    if mp:
        out['phone'] = mp.group(0)
    # location: look for 'from CITY' or 'in CITY'
    ml = re.search(r"\b(?:from|in)\s+([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)", text)
    if ml:
        out['location'] = ml.group(1).strip()
    # age: numbers followed by 'years' or 'yrs' or pattern 'I am 30'
    ma = re.search(r"(\b\d{1,3}\b)\s*(?:years old|years|yrs|y/o)?", text)
    if ma:
        maybe_age = int(ma.group(1))
        if 0 < maybe_age < 120:
            out['age'] = maybe_age

    return out

# Validator against the simple JSON_SCHEMA
def validate_extraction(extracted: Dict[str, Any]) -> Dict[str, Any]:
    """Return a dict with keys: valid (bool), errors (list), cleaned (dict)"""
    errors = []
    cleaned = {}
    # name: ensure string
    name = extracted.get('name')
    if name:
        cleaned['name'] = str(name).strip()
    # email
    email = extracted.get('email')
    if email:
        if re.match(r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$", email):
            cleaned['email'] = email
        else:
            errors.append('email invalid format')
    # phone: basic digits check
    phone = extracted.get('phone')
    if phone:
        digits = re.sub(r"\D", '', phone)
        if 7 <= len(digits) <= 15:
            cleaned['phone'] = phone
        else:
            errors.append('phone invalid length')
    # location
    location = extracted.get('location')
    if location:
        cleaned['location'] = str(location)
    # age
    age = extracted.get('age')
    if age is not None:
        try:
            ai = int(age)
            if 0 < ai < 120:
                cleaned['age'] = ai
            else:
                errors.append('age out of range')
        except Exception:
            errors.append('age not integer')

    return {'valid': len(errors) == 0, 'errors': errors, 'cleaned': cleaned}

# ----------------------------
# Demonstration helper for Task 2
# ----------------------------

def demo_task2():
    print('\n--- Task 2 Demo: JSON Schema Classification & Extraction ---')
    sample_chats = [
        "Hey, I'm Priya Sharma, 27 years old from Bengaluru. You can reach me at priya.sharma@example.com or +91 98765 43210.",
        "Hello, my name is John Doe. Email: john.doe@acme.co. I'm 34 and live in Austin.",
        "Hi, it's Maria. Call me at (555) 123-4567. I live in New York."
    ]

    for i, chat in enumerate(sample_chats, 1):
        print(f"\nSample chat #{i}: {chat}")
        extracted = extract_with_model(chat)
        print('Raw extracted:', json.dumps(extracted, indent=2))
        validated = validate_extraction(extracted)
        print('Validation result:', json.dumps(validated, indent=2))

# ----------------------------
# Run demonstrations if executed directly
# ----------------------------
if __name__ == '__main__':
    # Task 1 demonstration
    demo_task1()

    # Task 2 demonstration
    demo_task2()

    print('\n--- End of demo script.\nNotes:')
    print('- To run model-powered summarization/extraction, set GROQ_API_KEY environment variable or replace placeholder in the script.')
    print('- This file is ready to be uploaded to Google Colab as a single cell (or split into cells for readability).')
    print('- Commit this file to your GitHub repo for submission. Include a README describing usage and where to set API keys.')